
<!DOCTYPE html>
<html lang="fr">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Visualisation du Code</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <script src="https://unpkg.com/vue@3/dist/vue.global.js"></script>
    <!-- Prism.js CSS (Thème Okaidia) -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-okaidia.min.css" rel="stylesheet" />
    <style>
        body { font-family: sans-serif; }
        pre[class*="language-"] {
            margin: 0 0 0.5rem 0; /* Espace sous chaque pre */
            border-radius: 0.5rem; /* Correspond à rounded-lg */
            padding: 0.8rem;
             font-size: 0.875rem; /* text-sm */
             line-height: 1.4;
             overflow-x: auto; /* Scroll horizontal si besoin DANS le pre */
        }
         .column-content {
            /* Ce conteneur ne doit pas avoir de scroll vertical */
            display: flex;
            flex-direction: column;
            height: 100%; /* Occupe la hauteur de la cellule de grille */
            overflow: hidden; /* Empêche le contenu de déborder verticalement */
         }

         /* Ajustement pour que le code ne déborde pas horizontalement si trop long,
            mais PRISM gère souvent mieux avec white-space: pre par défaut + overflow-x: auto */
         code[class*="language-"] {
            /* white-space: pre-wrap; */ /* Alternative si overflow-x ne suffit pas */
            /* word-wrap: break-word; */
            display: block;
        }
        button:disabled {
            opacity: 0.5;
            cursor: not-allowed;
        }
        /* Hauteur minimale pour les pages pour éviter le "saut" de la pagination */
        .page-container { min-height: 75vh; }
    </style>
</head>
<body class="bg-gray-100 p-4">
 
<div id="app" >
   <div class="flex justify-center items-center space-x-4 mb-1">  <!-- Marge en bas pour espacer de la grille -->
        <button @click="prevPage" :disabled="currentPage === 0"
                class="bg-blue-600 hover:bg-blue-700 text-white font-bold py-1 px-4 rounded-md shadow transition duration-150 ease-in-out disabled:opacity-50 disabled:cursor-not-allowed">
            Précédent
        </button>
        <span class="text-gray-700 font-medium">Page {{ currentPage + 1 }} / {{ totalPages }}</span>
        <button @click="nextPage" :disabled="currentPage >= totalPages - 1"
                class="bg-blue-600 hover:bg-blue-700 text-white font-bold py-1 px-4 rounded-md shadow transition duration-150 ease-in-out disabled:opacity-50 disabled:cursor-not-allowed">
            Suivant
        </button>
    </div>

    <!-- Conteneur de la page actuelle -->
    <div class="grid grid-cols-3 gap-4 mb-4 page-container">
        <!-- Colonne -->
        <div v-for="(columnCells, colIdx) in currentPageData" :key="colIdx"
             :class="['p-2 rounded-lg shadow-md column-content', getColumnBgClass(colIdx)]">
             <!-- Cellule de code dans la colonne -->
             <pre v-for="(cellContent, cellIdx) in columnCells" :key="cellIdx"
                  class="shadow-inner bg-gray-800 text-white">
<code class="language-python" v-html="cellContent"></code></pre>
             <!-- Placeholder si colonne vide pour garder la structure -->
             <div v-if="columnCells.length === 0" class="h-full w-full flex-grow"></div>
        </div>
    </div>



</div>

<!-- Prism.js JS -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/autoloader/prism-autoloader.min.js" data-autoloader-path="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/"></script>
<!-- Charger explicitement Python n'est plus nécessaire avec l'autoloader bien configuré -->

<script>
    const { createApp, ref, computed, nextTick, watch } = Vue;

    // Les données des pages générées par Python
    const pagesData = [[["from google.colab import drive\ndrive.mount(&#x27;/content/drive&#x27;)", "# Biblioth\u00e8que pour lire les fichiers PD\n!pip install pymupdf", "# Lire le fichier texte brut\nwith open(&quot;/content/drive/MyDrive/pinocchio.txt&quot;, &quot;r&quot;, encoding=&quot;utf-8&quot;) as f:\n    raw = f.read()", "import re\n# Nettoyage de base\nraw = re.sub(r&#x27;Pinocchio\u2026\\s*\\d+&#x27;, &#x27;&#x27;, raw)  # Supprime les titres avec num\u00e9ros de page\nraw = re.sub(r&#x27;\\n+&#x27;, &#x27;\\n&#x27;, raw).strip()     # Supprime les lignes vides multiples", "# Importation de la biblioth\u00e8que NLTK (Natural Language Toolkit)\nimport nltk\n# T\u00e9l\u00e9chargement des donn\u00e9es n\u00e9cessaires pour le tokeniseur\nnltk.download(&#x27;punkt&#x27;)\nnltk.download(&#x27;punkt_tab&#x27;)\nnltk.download(&#x27;stopwords&#x27;)\nnltk.download(&#x27;wordnet&#x27;)\nnltk.download(&#x27;omw-1.4&#x27;)\nnltk.download(&#x27;averaged_perceptron_tagger&#x27;)", "# Tokenisation du texte brut (variable &#x27;raw&#x27;) en mots individuels (tokens)\ntokens = nltk.word_tokenize(raw)\n# Affichage des 20 premiers tokens pour v\u00e9rifier le r\u00e9sultat\nprint(&quot;Les 20 premiers tokens extraits du texte&quot;, tokens[:20])\n# Cr\u00e9ation d&#x27;un objet Text de NLTK \u00e0 partir des tokens pour des analyses textuelles plus avanc\u00e9es\ntext = nltk.Text(tokens)\n# D\u00e9tection et affichage des collocations (expressions fr\u00e9quentes de mots associ\u00e9s)\ntext.collocations()\n# Importation de la classe FreqDist pour calculer la fr\u00e9quence des mots\nfrom nltk.probability import FreqDist\n# Cr\u00e9ation de la distribution de fr\u00e9quence des tokens\nfdist = FreqDist(tokens)\n# Affichage d\u2019un graphique cumulatif des 30 mots les plus fr\u00e9quents\nfdist.plot(30, cumulative=True)"], ["from nltk.tokenize import word_tokenize, sent_tokenize\nfrom itertools import islice\n# Tokenisation des phrases\nsentences = sent_tokenize(raw, language=&quot;french&quot;)\n# Liste des mots (tokenisation simple)\nwords = [word for sentence in sentences for word in sentence.split()]\n# Cr\u00e9ation des n-grammes\nunigrams = list(set(words))\nbigrams = list(zip(words, words[1:]))\ntrigrams = list(zip(words, words[1:], words[2:]))\n# Statistiques de base\nprint(&quot;Nombre de phrases :&quot;, len(sentences))\nprint(&quot;Nombre de mots :&quot;, len(words))\nprint(&quot;Nombre de mots uniques (unigrammes) :&quot;, len(unigrams))\nprint(&quot;Nombre de bigrammes :&quot;, len(bigrams))\nprint(&quot;Nombre de trigrammes :&quot;, len(trigrams))\n# Affichage d&#x27;exemples\nprint(&quot;Exemples de phrases :&quot;)\nfor sent in sentences[:3]:\n    print(&quot;-&quot;, sent)\nprint(&quot;Exemples de bigrammes :&quot;)\nfor pair in islice(bigrams, 5):\n    print(&quot;-&quot;, pair)", "import re\n# Cr\u00e9er une liste de mots se terminant par &quot;ion&quot;\nwords_ion = [w for w in tokens if re.search(&#x27;ion$&#x27;, w)]\nprint(words_ion[:30])", "import matplotlib.pyplot as plt\nfrom collections import Counter\n# Longueur de chaque mot dans tout le texte\nlongueurs_mots = [len(m) for m in tokens]\n# Histogramme des longueurs de mots\nCounter(longueurs_mots).most_common()\nplt.hist(longueurs_mots, bins=range(1, max(longueurs_mots)+1))\nplt.title(&quot;Distribution des longueurs de mots&quot;)\nplt.xlabel(&quot;Longueur du mot&quot;)\nplt.ylabel(&quot;Fr\u00e9quence&quot;)\nplt.show()", "# Calcul de la longueur moyenne des phrases\ndef simple_tokenize(text):\n    return re.findall(r&quot;\\b\\w+\\b&quot;, text)\n# D\u00e9coupe brute en phrases\nphrases_brutes = re.split(r&#x27;[.!?]&#x27;, raw)\nlongueurs_phrases = [len(simple_tokenize(p)) for p in phrases_brutes if p.strip()]\nlongueur_moyenne_phrase = sum(longueurs_phrases) / len(longueurs_phrases)\nlongueur_moyenne_phrase", "# Filtrage des mots de contenu (en supprimant des mots vides simples)\nstopwords = [&#x27;the&#x27;, &#x27;and&#x27;, &#x27;of&#x27;, &#x27;to&#x27;, &#x27;a&#x27;, &#x27;in&#x27;, &#x27;is&#x27;, &#x27;that&#x27;, &#x27;,&#x27;, &#x27;.&#x27;, &#x27;!&#x27;, &#x27;?&#x27;]\ncontent_words = [w for w in tokens if w.lower() not in stopwords]\nprint(&quot;Mots de contenu :&quot;, content_words[:20])"], ["from nltk.stem import PorterStemmer, WordNetLemmatizer\nstemmer = PorterStemmer()\nlemmatizer = WordNetLemmatizer()\n# Appliquer stemming et lemmatisation\nstems = [stemmer.stem(w) for w in tokens]\nlemmes = [lemmatizer.lemmatize(w) for w in tokens]\n# Statistiques\nprint(&quot;Nombre de stems uniques :&quot;, len(set(stems)))\nprint(&quot;Nombre de lemmes uniques :&quot;, len(set(lemmes)))\n# Fr\u00e9quence des lemmes\nfrom nltk.probability import FreqDist\nfdist_lem = FreqDist(lemmes)\nfdist_lem.plot(30, title=&quot;Fr\u00e9quence des lemmes les plus fr\u00e9quents&quot;)", "#pip install transformers", "# D\u00e9couper le texte en petits morceaux pour la traduction\ndef decouper_texte(text, longueur_max=400):\n    phrases = text.split(&#x27;.&#x27;)\n    morceaux = []\n    courant = &#x27;&#x27;\n    for phrase in phrases:\n        if len(courant) + len(phrase) &lt; longueur_max:\n            courant += phrase + &#x27;.&#x27;\n        else:\n            morceaux.append(courant.strip())\n            courant = phrase + &#x27;.&#x27;\n    if courant:\n        morceaux.append(courant.strip())\n    return morceaux\nmorceaux = decouper_texte(raw)", "# 2. Charger le mod\u00e8le de traduction anglais \u2192 fran\u00e7ais depuis Hugging Face\nfrom transformers import MarianTokenizer, MarianMTModel\nmodele_nom = &quot;Helsinki-NLP/opus-mt-en-fr&quot;\ntokenizer = MarianTokenizer.from_pretrained(modele_nom)\nmodele = MarianMTModel.from_pretrained(modele_nom)", "# Tester la traduction sur une seule phrase\nphrase_test = raw.split(&#x27;.&#x27;)[0].strip() + &quot;.&quot;\nentrees = tokenizer(phrase_test, return_tensors=&quot;pt&quot;, padding=True, truncation=True)\nsortie = modele.generate(**entrees)\nphrase_traduite = tokenizer.batch_decode(sortie, skip_special_tokens=True)[0]\nprint(&quot; Phrase originale :\\n&quot;, phrase_test)\nprint(&quot;\\n Traduction en fran\u00e7ais :\\n&quot;, phrase_traduite)", "# 3. Traduire chaque morceau\n#traductions = []\n#for morceau in morceaux:\n    #entrees = tokenizer(morceau, return_tensors=&quot;pt&quot;, padding=True, truncation=True)\n    #sortie = modele.generate(**entrees)\n    #texte_fr = tokenizer.batch_decode(sortie, skip_special_tokens=True)[0]\n    #traductions.append(texte_fr)"]], [["# 4. Recomposer le texte traduit complet\n#texte_traduit = &quot;\\n\\n&quot;.join(traductions)\n# 5. (Optionnel) Sauvegarder dans un fichier texte\n#with open(&quot;/content/drive/MyDrive/pinocchio_traduit_fr.txt&quot;, &quot;w&quot;, encoding=&quot;utf-8&quot;) as f:\n    #f.write(texte_traduit)\n#print(&quot;Traduction termin\u00e9e avec succ\u00e8s !&quot;)", "# Pr\u00e9paration des donn\u00e9es pour TF-IDF\ndocuments = [raw]\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport pandas as pd\n# Cr\u00e9ation du mod\u00e8le TF-IDF (en utilisant les stop words en anglais de sklearn)\nvectorizer = TfidfVectorizer(stop_words=&#x27;english&#x27;, max_features=20)  # limitation aux 20 mots ayant les scores TF-IDF les plus \u00e9lev\u00e9s\nmatrice_tfidf = vectorizer.fit_transform(documents)\n# Extraction du vocabulaire et des scores TF-IDF associ\u00e9s\nnoms_des_mots = vectorizer.get_feature_names_out()\nscores_tfidf = matrice_tfidf.toarray().flatten()\n# Cr\u00e9ation d&#x27;un DataFrame avec le vocabulaire et les scores TF-IDF\ndf_tfidf = pd.DataFrame({\n    &#x27;Vocabulaire&#x27;: noms_des_mots,\n    &#x27;TF-IDF&#x27;: scores_tfidf\n}).sort_values(by=&#x27;TF-IDF&#x27;, ascending=False)\n# Affichage des r\u00e9sultats\nprint(df_tfidf)"], ["from sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.decomposition import LatentDirichletAllocation\n# 1. Nettoyage l\u00e9ger du texte brut\ndef nettoyer_texte(texte):\n    texte = texte.lower()\n    texte = re.sub(r&#x27;\\d+&#x27;, &#x27;&#x27;, texte)\n    texte = re.sub(r&#x27;[^\\w\\s]&#x27;, &#x27;&#x27;, texte)\n    texte = re.sub(r&#x27;\\s+&#x27;, &#x27; &#x27;, texte)\n    return texte\ntexte_nettoye = nettoyer_texte(raw)\n# 2. D\u00e9coupage du texte en paragraphes ou phrases\ndocuments = texte_nettoye.split(&#x27;. &#x27;)\n# Transformer les documents en une matrice document-terme\nvectorizer = CountVectorizer(stop_words=&#x27;english&#x27;, token_pattern=r&#x27;\\b[a-zA-Z]{4,}\\b&#x27;)\nmatrice_doc_terme = vectorizer.fit_transform(documents)\n# Cr\u00e9ation du mod\u00e8le LDA pour identifier 3 sujets latents (n_topics=3)\nnombre_de_sujets = 3\nmodele_lda = LatentDirichletAllocation(n_components=nombre_de_sujets, random_state=42)\nmodele_lda.fit(matrice_doc_terme)\n# Fonction d&#x27;affichage des r\u00e9sultats des sujets\ndef afficher_sujets(modele, noms_caracteristiques, nombre_mots):\n    for index, sujet in enumerate(modele.components_):\n        print(f&quot;Sujet #{index + 1}:&quot;)\n        indices_mots = sujet.argsort()[:-nombre_mots - 1:-1]\n        mots_importants = [noms_caracteristiques[i] for i in indices_mots]\n        poids_mots = sujet[indices_mots]\n        print(&quot; | &quot;.join([f&quot;{mot} ({poids:.2f})&quot; for mot, poids in zip(mots_importants, poids_mots)]))\n        print()\n# Afficher les 5 mots les plus repr\u00e9sentatifs pour chaque sujet\nnombre_mots = 10\nnoms_caracteristiques = vectorizer.get_feature_names_out()\nprint(&quot;Les sujets latents extraits du texte \u00ab Pinocchio \u00bb :\\n&quot;)\nafficher_sujets(modele_lda, noms_caracteristiques, nombre_mots)\n# Afficher la distribution des sujets dans chaque paragraphe (les 500 premiers seulement)\ndistribution_sujets_documents = modele_lda.transform(matrice_doc_terme)\nfor i, distribution in enumerate(distribution_sujets_documents[:500]):\n    info_sujet = &quot; | &quot;.join([f&quot;Sujet {j+1}: {prob:.2f}&quot; for j, prob in enumerate(distribution)])\n    print(f&quot;Paragraphe {i+1}: {info_sujet}\\n&quot;)"], ["from textblob import TextBlob\n# Nettoyage du texte brut\ndef nettoyer_texte(texte):\n    texte = texte.replace(&#x27;\\xa0&#x27;, &#x27; &#x27;)\n    texte = re.sub(r&#x27;\\s+&#x27;, &#x27; &#x27;, texte)\n    return texte.strip()\ntexte_propre = nettoyer_texte(raw)\n# D\u00e9coupage du texte en phrases\nphrases = sent_tokenize(texte_propre)\n# Liste des personnages \u00e0 suivre\npersonnages = [&#x27;pinocchio&#x27;, &#x27;geppetto&#x27;, &#x27;fairy&#x27;, &#x27;cricket&#x27;, &#x27;fox&#x27;, &#x27;cat&#x27;]\n# Analyse de chaque phrase\nresultats = []\nfor phrase in phrases:\n    phrase_minuscule = phrase.lower()\n    personnages_trouves = [p for p in personnages if p in phrase_minuscule]\n    if personnages_trouves:  # uniquement si un personnage est mentionn\u00e9\n        polarite = TextBlob(phrase).sentiment.polarity\n        sentiment = &quot;Positif&quot; if polarite &gt; 0.1 else &quot;N\u00e9gatif&quot; if polarite &lt; -0.1 else &quot;Neutre&quot;\n        for personnage in personnages_trouves:\n            resultats.append({\n                &quot;Phrase&quot;: phrase,\n                &quot;Personnage&quot;: personnage.capitalize(),\n                &quot;Sentiment&quot;: sentiment,\n                &quot;Score&quot;: round(polarite, 3)\n            })\n# Cr\u00e9ation du tableau de r\u00e9sultats\ndf_textblob = pd.DataFrame(resultats)\n# Affichage des premi\u00e8res lignes\nprint(df_textblob.head())\n# Exporter les r\u00e9sultats vers un fichier CSV\ndf_textblob.to_csv(&quot;sentiments_par_personnage.csv&quot;, index=False)", "from collections import Counter\n# Cr\u00e9er une liste de tuples (Personnage, Sentiment)\ncombinaisons = list(zip(df_textblob[&quot;Personnage&quot;], df_textblob[&quot;Sentiment&quot;]))\n# Compter les occurrences\ncompte = Counter(combinaisons)\n# Affichage des r\u00e9sultats\nprint(&quot;R\u00e9partition des sentiments par personnage :&quot;)\nfor (personnage, sentiment), nb in compte.items():\n    print(f&quot;{personnage} - {sentiment} : {nb} phrases&quot;)"]], [["from nltk import word_tokenize\nfrom nltk.classify import NaiveBayesClassifier\n# D\u1eef li\u1ec7u hu\u1ea5n luy\u1ec7n \u0111\u01a1n gi\u1ea3n (b\u1ea1n c\u00f3 th\u1ec3 m\u1edf r\u1ed9ng)\ntrain_data = [\n    (&quot;I love this!&quot;, &quot;Positif&quot;),\n    (&quot;What a wonderful scene.&quot;, &quot;Positif&quot;),\n    (&quot;This is terrible.&quot;, &quot;N\u00e9gatif&quot;),\n    (&quot;I hate that character.&quot;, &quot;N\u00e9gatif&quot;),\n    (&quot;Nothing much happened.&quot;, &quot;Neutre&quot;),\n    (&quot;It was okay, not great.&quot;, &quot;Neutre&quot;)\n]\ndef extract_features(text):\n    words = word_tokenize(text)\n    return {f&quot;contains({w.lower()})&quot;: True for w in words}\ntrain_set = [(extract_features(text), label) for (text, label) in train_data]\nnb_classifier = NaiveBayesClassifier.train(train_set)", "# Analyse avec Naive Bayes\nresultats_nb = []\nfor phrase in phrases:\n    phrase_minuscule = phrase.lower()\n    personnages_trouves = [p for p in personnages if p in phrase_minuscule]\n    if personnages_trouves:\n        features = extract_features(phrase)\n        sentiment_nb = nb_classifier.classify(features)\n        for personnage in personnages_trouves:\n            resultats_nb.append({\n                &quot;Phrase&quot;: phrase,\n                &quot;Personnage&quot;: personnage.capitalize(),\n                &quot;Sentiment_NaiveBayes&quot;: sentiment_nb\n            })\n# Cr\u00e9er DataFrame et sauvegarder\ndf_nb = pd.DataFrame(resultats_nb)\nprint(df_nb.head())\ndf_nb.to_csv(&quot;sentiments_naivebayes.csv&quot;, index=False)", "# Fusionner les deux r\u00e9sultats sur Phrase + Personnage\ndf_comparaison = pd.merge(df_textblob, df_nb, on=[&quot;Phrase&quot;, &quot;Personnage&quot;])\nprint(df_comparaison.head())\n# Sauvegarder\ndf_comparaison.to_csv(&quot;comparaison_sentiments_textblob_vs_nb.csv&quot;, index=False)", "#Calcul du pourcentage d\u2019accord entre les deux mod\u00e8les\n# Comparer les sentiments pr\u00e9dits\ndf_comparaison[&quot;M\u00eame sentiment&quot;] = df_comparaison[&quot;Sentiment&quot;] == df_comparaison[&quot;Sentiment_NaiveBayes&quot;]\n# Calculer le pourcentage d\u2019accord\naccord = df_comparaison[&quot;M\u00eame sentiment&quot;].mean() * 100\nprint(f&quot;Pourcentage d\u2019accord entre TextBlob et Naive Bayes : {accord:.2f} %&quot;)"], ["#Matrice de confusion\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n# Vrais sentiments (TextBlob) vs pr\u00e9dictions (Naive Bayes)\ny_true = df_comparaison[&quot;Sentiment&quot;]\ny_pred = df_comparaison[&quot;Sentiment_NaiveBayes&quot;]\n# D\u00e9finir les \u00e9tiquettes\nlabels = [&quot;Positif&quot;, &quot;Neutre&quot;, &quot;N\u00e9gatif&quot;]\n# Matrice de confusion\ncm = confusion_matrix(y_true, y_pred, labels=labels)\n# Affichage graphique\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\ndisp.plot(cmap=&quot;Blues&quot;, xticks_rotation=45)\nplt.title(&quot;Matrice de confusion : TextBlob vs Naive Bayes&quot;)\nplt.show()", "import fitz\nfrom sklearn.linear_model import LogisticRegression\nexemples = [\n    (&quot;Pinocchio said he was sorry.&quot;, &quot;parole&quot;),\n    (&quot;Geppetto ran after him.&quot;, &quot;action physique&quot;),\n    (&quot;He felt very sad and cried.&quot;, &quot;\u00e9motion&quot;),\n    (&quot;The sun was shining above the hills.&quot;, &quot;description&quot;),\n    (&quot;The Fairy asked him why he lied.&quot;, &quot;parole&quot;),\n    (&quot;He jumped over the wall.&quot;, &quot;action physique&quot;),\n    (&quot;He was afraid of being punished.&quot;, &quot;\u00e9motion&quot;),\n    (&quot;There was a silence in the room.&quot;, &quot;description&quot;)\n]\nX_entrainement_textes = [x[0] for x in exemples]\ny_entrainement = [x[1] for x in exemples]\n# Vectorisation des textes\nvectorizer = TfidfVectorizer()\nX_entrainement = vectorizer.fit_transform(X_entrainement_textes)\n# Entra\u00eenement du mod\u00e8le\nmodele = LogisticRegression()\nmodele.fit(X_entrainement, y_entrainement)\n# Pr\u00e9diction du type d\u2019action sur l\u2019ensemble du texte\nX_complet = vectorizer.transform(phrases)\npredictions = modele.predict(X_complet)\n# Affichage des r\u00e9sultats\ndf_resultat = pd.DataFrame({\n    &quot;Phrase&quot;: phrases,\n    &quot;Type d&#x27;action&quot;: predictions\n})\n# Afficher les 10 premi\u00e8res lignes\nprint(df_resultat.head(10))", "compte = Counter(predictions)\nprint(&quot;R\u00e9partition des types d&#x27;action :&quot;)\nfor action, nb in compte.items():\n    print(f&quot;{action.capitalize()} : {nb} phrases&quot;)"], ["nltk.download(&#x27;punkt&#x27;)\nnltk.download(&#x27;averaged_perceptron_tagger&#x27;)\nnltk.download(&#x27;averaged_perceptron_tagger_eng&#x27;)\nnltk.download(&#x27;maxent_ne_chunker&#x27;)\nnltk.download(&#x27;words&#x27;)\nfrom nltk import word_tokenize, pos_tag, RegexpParser, sent_tokenize\n# D\u00e9finir une grammaire pour rep\u00e9rer les groupes nominaux\ngrammaire = &quot;NP: {&lt;DT&gt;?&lt;JJ&gt;*&lt;NN|NNS|NNP|NNPS&gt;+}&quot;\nanalyseur = RegexpParser(grammaire)\n# Exemple sur une phrase isol\u00e9e\nphrase_exemple = &quot;The good fairy gave Pinocchio a piece of advice.&quot;\ntokens_exemple = word_tokenize(phrase_exemple)\ntags_exemple = pos_tag(tokens_exemple)\narbre_exemple = analyseur.parse(tags_exemple)\nprint(&quot;\\nAnalyse syntaxique de la phrase exemple :&quot;)\narbre_exemple.pretty_print()\n# Application sur tout le texte ----\nphrases = sent_tokenize(raw)\ngroupes_nominaux = []\nfor phrase in phrases:\n    tokens = word_tokenize(phrase)\n    tags = pos_tag(tokens)\n    arbre = analyseur.parse(tags)\n    for sous_arbre in arbre.subtrees():\n        if sous_arbre.label() == &#x27;NP&#x27;:\n            groupe = &quot; &quot;.join(mot for mot, tag in sous_arbre.leaves())\n            groupes_nominaux.append(groupe)\nprint(&quot;\\nExemples de groupes nominaux extraits dans le texte :&quot;)\nfor g in groupes_nominaux[:20]:\n    print(&quot;-&quot;, g)\nprint(f&quot;\\nNombre total de groupes nominaux d\u00e9tect\u00e9s : {len(groupes_nominaux)}&quot;)", "import string\nimport nltk\nfrom nltk.tokenize import word_tokenize, sent_tokenize\nfrom nltk.corpus import stopwords\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom transformers import pipeline\nnltk.download(&#x27;punkt&#x27;)\nnltk.download(&#x27;stopwords&#x27;)\n# Fonction de pr\u00e9traitement du texte\nstop_words = set(stopwords.words(&#x27;english&#x27;))\ndef nettoyer_texte(text):\n    text = text.lower()\n    text = &#x27;&#x27;.join([c for c in text if c not in string.punctuation])\n    tokens = word_tokenize(text)\n    tokens = [w for w in tokens if w not in stop_words]\n    return &#x27; &#x27;.join(tokens)"]], [["# Fonction pour trouver la phrase la plus pertinente avec TF-IDF\ndef repondre_tfidf(question, corpus):\n    corpus_nettoye = [nettoyer_texte(sent) for sent in corpus]\n    question_nettoyee = nettoyer_texte(question)\n    vect = TfidfVectorizer()\n    matrice_tfidf = vect.fit_transform(corpus_nettoye + [question_nettoyee])\n    vect_question = matrice_tfidf[-1]\n    vect_docs = matrice_tfidf[:-1]\n    similarites = cosine_similarity(vect_question, vect_docs)[0]\n    meilleur_index = similarites.argmax()\n    return corpus[meilleur_index], similarites[meilleur_index], meilleur_index", "# Fonction de r\u00e9ponse avec Hugging Face\nmodele_qa = pipeline(&quot;question-answering&quot;)\ndef repondre_huggingface(question, contexte):\n    try:\n        resultat = modele_qa(question=question, context=contexte)\n        return resultat[&quot;answer&quot;], resultat[&quot;score&quot;]\n    except:\n        return &quot;Je ne sais pas&quot;, 0.0", "# Exemple de question\nquestion = &quot;Qui est Geppetto ?&quot;\n# retrouver la phrase la plus proche\nphrase_pertinente, score_tfidf, index_phrase = repondre_tfidf(question, phrases)\n# cr\u00e9er un meilleur contexte si la phrase est trop courte\nif len(phrase_pertinente.split()) &lt; 8:\n    contexte = &quot; &quot;.join(phrases[index_phrase:index_phrase + 3])\nelse:\n    contexte = phrase_pertinente\n# extraction de la r\u00e9ponse\nreponse, score_hf = repondre_huggingface(question, contexte)\n# R\u00e9sultats\nprint(&quot;M\u00e9thode TF-IDF :&quot;)\nprint(&quot;\u2192 Phrase la plus pertinente :&quot;, phrase_pertinente)\nprint(&quot;\u2192 Similarit\u00e9 :&quot;, round(score_tfidf, 3))\nprint(&quot;\\nMod\u00e8le Hugging Face :&quot;)\nprint(&quot;\u2192 R\u00e9ponse extraite :&quot;, reponse)\nprint(&quot;\u2192 Score de confiance :&quot;, round(score_hf, 3))"], ["# R\u00e9sum\u00e9 extractif avec CamemBERT\nimport numpy as np\nimport torch\nfrom transformers import AutoTokenizer, AutoModel\nfrom numpy.linalg import norm\n# Chargement du tokenizer et du mod\u00e8le CamemBERT\nmodel_name = &quot;camembert-base&quot;\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModel.from_pretrained(model_name)\n# Fonction pour obtenir l&#x27;embedding d&#x27;une phrase\ndef get_sentence_embedding(phrase):\n    inputs = tokenizer(phrase, return_tensors=&quot;pt&quot;, truncation=True, max_length=512)\n    with torch.no_grad():\n        outputs = model(**inputs)\n    return outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n# Embeddings des phrases\nembeddings = [get_sentence_embedding(p) for p in phrases]\ndoc_embedding = np.mean(embeddings, axis=0)\n# Fonction de similarit\u00e9 cosinus\ndef cosine_similarity(v1, v2):\n    return np.dot(v1, v2) / (norm(v1) * norm(v2))\n# Score de chaque phrase\nscores = [cosine_similarity(e, doc_embedding) for e in embeddings]\n# S\u00e9lection des meilleures phrases (top 30 %)\nnb_phrases = max(1, int(0.3 * len(phrases)))\nindices = np.argsort(scores)[-nb_phrases:]\nindices = sorted(indices)\n# R\u00e9sum\u00e9 final\nresume = [phrases[i] for i in indices]\n# Affichage\nprint(&quot;R\u00e9sum\u00e9 automatique avec CamemBERT :\\n&quot;)\nfor ligne in resume:\n    print(&quot;\u2022&quot;, ligne)", "# S\u00e9parer le texte en chapitres\nchapters = re.split(r&#x27;CHAPTER \\d+&#x27;, raw)\nchapter_titles = re.findall(r&#x27;CHAPTER \\d+&#x27;, raw)\n# Associer chaque titre de chapitre avec son contenu\nchapter_dicts = []\nfor title, content in zip(chapter_titles, chapters[1:]):  # skip the first empty split\n    chapter_dicts.append({&quot;title&quot;: title.strip(), &quot;content&quot;: content.strip()})"], ["from transformers import BartTokenizer, BartForConditionalGeneration\n# Chargement du mod\u00e8le de r\u00e9sum\u00e9 BART\ntokenizer = BartTokenizer.from_pretrained(&quot;facebook/bart-large-cnn&quot;)\nmodel = BartForConditionalGeneration.from_pretrained(&quot;facebook/bart-large-cnn&quot;)\n# Fonction de r\u00e9sum\u00e9 automatique\ndef summarize(text):\n    inputs = tokenizer(text, return_tensors=&quot;pt&quot;, max_length=1024, truncation=True)\n    summary_ids = model.generate(\n        inputs[&quot;input_ids&quot;],\n        max_length=150,\n        min_length=60,\n        length_penalty=2.0,\n        num_beams=4,\n        early_stopping=True\n    )\n    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)", "for i, chapter in enumerate(chapter_dicts):\n    print(f&quot;{chapter[&#x27;title&#x27;]}&quot;)\n    summary = summarize(chapter[&#x27;content&#x27;])\n    print(f&quot;Summary:\\n{summary}\\n&quot;)", "pip install gradio"]], [["import gradio as gr\nfrom transformers import pipeline\n# 1. Pipelines\nqa_pipeline = pipeline(&quot;question-answering&quot;)\nsummarizer = pipeline(&quot;summarization&quot;, model=&quot;facebook/bart-large-cnn&quot;)\ntranslator = pipeline(&quot;translation_en_to_fr&quot;, model=&quot;Helsinki-NLP/opus-mt-en-fr&quot;)\nsentiment_pipeline = pipeline(&quot;sentiment-analysis&quot;)\n# 2. Fonctions\ndef poser_question(question):\n    contexte = raw[:1500]  # ou r\u00e9sume le texte ou utilise des phrases pertinentes\n    reponse = qa_pipeline(question=question, context=contexte)\n    return reponse[&quot;answer&quot;]\ndef resumer_texte(input_text):\n    resume = summarizer(input_text, max_length=130, min_length=30, do_sample=False)\n    return resume[0][&quot;summary_text&quot;]\ndef analyser_sentiment(phrase):\n    resultat = sentiment_pipeline(phrase)[0]\n    return f&quot;{resultat[&#x27;label&#x27;]} (score: {round(resultat[&#x27;score&#x27;], 2)})&quot;\ndef traduire_en_fr(phrase):\n    trad = translator(phrase)\n    return trad[0][&#x27;translation_text&#x27;]\n# 3. Interface Gradio\ndemo = gr.Interface(\n    fn=poser_question,\n    inputs=gr.Textbox(label=&quot;Pose une question sur Pinocchio&quot;),\n    outputs=gr.Textbox(label=&quot;R\u00e9ponse&quot;),\n    title=&quot;Pinocchio QA - Gradio&quot;,\n    description=&quot;Pose une question sur le conte, obtiens une r\u00e9ponse !&quot;\n)\ndemo2 = gr.Interface(fn=resumer_texte, inputs=&quot;text&quot;, outputs=&quot;text&quot;, title=&quot;R\u00e9sum\u00e9 de texte&quot;)\ndemo3 = gr.Interface(fn=analyser_sentiment, inputs=&quot;text&quot;, outputs=&quot;text&quot;, title=&quot;Analyse de sentiment&quot;)\ndemo4 = gr.Interface(fn=traduire_en_fr, inputs=&quot;text&quot;, outputs=&quot;text&quot;, title=&quot;Traduction anglais \u2192 fran\u00e7ais&quot;)\n# 4. Regrouper tout dans un onglet\ngr.TabbedInterface(\n    [demo, demo2, demo3, demo4],\n    [&quot;QA&quot;, &quot;R\u00e9sum\u00e9&quot;, &quot;Sentiment&quot;, &quot;Traduction&quot;]\n).launch()"], [], []]]; // Injection JSON

    createApp({
        data() {
            return {
                pages: pagesData,
                currentPage: 0
            };
        },
        computed: {
            totalPages() {
                return this.pages.length > 0 ? this.pages.length : 1; // Au moins 1 page même si vide
            },
            currentPageData() {
                 // Gère le cas où il n'y a pas de pages (totalPages serait 1 mais this.pages[0] serait undefined)
                if (!this.pages || this.pages.length === 0) {
                     return [[], [], []]; // Structure vide à 3 colonnes
                }
                // Assure que même la dernière page a toujours 3 colonnes (potentiellement vides)
                let page = this.pages[this.currentPage] || []; // Prend la page ou un tableau vide si index hors limites
                while (page.length < 3) {
                    page.push([]);
                }
                return page.slice(0, 3); // Assure qu'il n'y a que NUM_COLUMNS colonnes
            }
        },
        methods: {
            nextPage() {
                if (this.currentPage < this.totalPages - 1) {
                    this.currentPage++;
                }
            },
            prevPage() {
                if (this.currentPage > 0) {
                    this.currentPage--;
                }
            },
            highlightCode() {
                // Attendre que le DOM soit mis à jour par Vue
                nextTick(() => {
                    // Utiliser Prism pour colorer tous les blocs de code sur la page actuelle
                    Prism.highlightAll(this.$el); // Cible l'élément racine de l'app Vue
                    // console.log("Prism highlighting applied on page " + (this.currentPage + 1));
                });
            },
            getColumnBgClass(index) {
              // Des fonds légèrement différents pour distinguer les colonnes
              const colors = ['bg-gray-50', 'bg-blue-50', 'bg-green-50'];
              // Si vous préférez la même couleur : ['bg-white', 'bg-white', 'bg-white']
              return colors[index % colors.length];
            }
        },
        mounted() {
            this.highlightCode(); // Coloration initiale
        },
         update() {
            this.highlightCode(); // Coloration initiale
        },
        watch: {
            // Re-colorer quand la page change
            currentPage() {
                this.highlightCode();
            }
        }
    }).mount('#app');

</script>

</body>
</html>

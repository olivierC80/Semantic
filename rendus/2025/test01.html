<!DOCTYPE html>
<html lang="fr">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Code Python avec Tailwind + Prism</title>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css" crossorigin="anonymous">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js" crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js" crossorigin="anonymous"
        onload="renderMathInElement(document.body);"></script>

  <!-- Tailwind CSS CDN -->
  <script src="https://cdn.tailwindcss.com"></script>

  <!-- Prism.js CSS + Python support -->
  <link href="https://cdn.jsdelivr.net/npm/prismjs@1/themes/prism-tomorrow.css" rel="stylesheet" />
  <script src="https://cdn.jsdelivr.net/npm/prismjs@1/prism.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/prismjs@1/components/prism-python.min.js"></script>

</head>
<body class="bg-gray-900 text-white p-2">



  <div class="grid grid-cols-3 gap-2">    
    <div class="bg-blue-200 p-2">
      <pre class="rounded-xl overflow-x-auto shadow-lg"><code class="language-python">from google.colab import drive
drive.mount('/content/drive')</code>        <code class="language-python">
# Bibliothèque pour lire les fichiers PDF
!pip install pymupdf</code><code class="language-python">
# Lire le fichier texte brut
with open("/content/drive/MyDrive/pinocchio.txt", "r", encoding="utf-8") as f:
  raw = f.read()</code>  <code class="language-python">
import re
# Nettoyage de base
raw = re.sub(r'Pinocchio…\s*\d+', '', raw)  # Supprime les titres avec numéros de page
raw = re.sub(r'\n+', '\n', raw).strip()     # Supprime les lignes vides multiples</code>  <code class="language-python">
# Importation de la bibliothèque NLTK (Natural Language Toolkit)
import nltk
# Téléchargement des données nécessaires pour le tokeniseur
nltk.download('punkt')
nltk.download('punkt_tab')
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('omw-1.4')
nltk.download('averaged_perceptron_tagger')</code><code class="language-python">
# Tokenisation du texte brut (variable 'raw') en mots individuels (tokens)
tokens = nltk.word_tokenize(raw)
# Affichage des 20 premiers tokens pour vérifier le résultat
print("Les 20 premiers tokens extraits du texte", tokens[:20])
# Création d'un objet Text de NLTK à partir des tokens pour des analyses textuelles plus avancées
text = nltk.Text(tokens)
# Détection et affichage des collocations (expressions fréquentes de mots associés)
text.collocations()
# Importation de la classe FreqDist pour calculer la fréquence des mots
from nltk.probability import FreqDist
# Création de la distribution de fréquence des tokens
fdist = FreqDist(tokens)
# Affichage d’un graphique cumulatif des 30 mots les plus fréquents
fdist.plot(30, cumulative=True)
from nltk.tokenize import word_tokenize, sent_tokenize
from itertools import islice
# Tokenisation des phrases
sentences = sent_tokenize(raw, language="french")
# Liste des mots (tokenisation simple)
words = [word for sentence in sentences for word in sentence.split()]
# Création des n-grammes
unigrams = list(set(words))
bigrams = list(zip(words, words[1:]))
trigrams = list(zip(words, words[1:], words[2:]))
# Statistiques de base
print("Nombre de phrases :", len(sentences))
print("Nombre de mots :", len(words))
print("Nombre de mots uniques (unigrammes) :", len(unigrams))
print("Nombre de bigrammes :", len(bigrams))
print("Nombre de trigrammes :", len(trigrams))
# Affichage d'exemples
print("Exemples de phrases :")
for sent in sentences[:3]:
    print("-", sent)
print("Exemples de bigrammes :")
for pair in islice(bigrams, 5):
    print("-", pair)</code></pre>

    </div>
    <div class="bg-blue-400 p-2">
      <pre class="rounded-xl overflow-x-auto shadow-lg "><code class="language-python">import re
# Créer une liste de mots se terminant par "ion"
words_ion = [w for w in tokens if re.search('ion$', w)]
print(words_ion[:30])
import matplotlib.pyplot as plt
from collections import Counter
# Longueur de chaque mot dans tout le texte
longueurs_mots = [len(m) for m in tokens]
# Histogramme des longueurs de mots
Counter(longueurs_mots).most_common()
plt.hist(longueurs_mots, bins=range(1, max(longueurs_mots)+1))
plt.title("Distribution des longueurs de mots")
plt.xlabel("Longueur du mot")
plt.ylabel("Fréquence")
plt.show()
# Calcul de la longueur moyenne des phrases
def simple_tokenize(text):
    return re.findall(r"\b\w+\b", text)
# Découpe brute en phrases
phrases_brutes = re.split(r'[.!?]', raw)
longueurs_phrases = [len(simple_tokenize(p)) for p in phrases_brutes if p.strip()]
longueur_moyenne_phrase = sum(longueurs_phrases) / len(longueurs_phrases)
longueur_moyenne_phrase
# Filtrage des mots de contenu (en supprimant des mots vides simples)
stopwords = ['the', 'and', 'of', 'to', 'a', 'in', 'is', 'that', ',', '.', '!', '?']
content_words = [w for w in tokens if w.lower() not in stopwords]
print("Mots de contenu :", content_words[:20])
from nltk.stem import PorterStemmer, WordNetLemmatizer
stemmer = PorterStemmer()
lemmatizer = WordNetLemmatizer()
# Appliquer stemming et lemmatisation
stems = [stemmer.stem(w) for w in tokens]
lemmes = [lemmatizer.lemmatize(w) for w in tokens]
# Statistiques
print("Nombre de stems uniques :", len(set(stems)))
print("Nombre de lemmes uniques :", len(set(lemmes)))
# Fréquence des lemmes
from nltk.probability import FreqDist
fdist_lem = FreqDist(lemmes)
fdist_lem.plot(30, title="Fréquence des lemmes les plus fréquents")
#pip install transformers
# Découper le texte en petits morceaux pour la traduction
def decouper_texte(text, longueur_max=400):
    phrases = text.split('.')
    morceaux = []
    courant = ''
    for phrase in phrases:
        if len(courant) + len(phrase) < longueur_max:
            courant += phrase + '.'
        else:
            morceaux.append(courant.strip())
            courant = phrase + '.'
    if courant:
        morceaux.append(courant.strip())
    return morceaux</code></pre>
    </div>
    <div class="bg-blue-500 p-2">
 <pre class="rounded-xl overflow-x-auto shadow-lg "><code class="language-python">morceaux = decouper_texte(raw)
# 2. Charger le modèle de traduction anglais → français depuis Hugging Face
from transformers import MarianTokenizer, MarianMTModel
modele_nom = "Helsinki-NLP/opus-mt-en-fr"
tokenizer = MarianTokenizer.from_pretrained(modele_nom)
modele = MarianMTModel.from_pretrained(modele_nom)
# Tester la traduction sur une seule phrase
phrase_test = raw.split('.')[0].strip() + "."
entrees = tokenizer(phrase_test, return_tensors="pt", padding=True, truncation=True)
sortie = modele.generate(**entrees)
phrase_traduite = tokenizer.batch_decode(sortie, skip_special_tokens=True)[0]
print(" Phrase originale :\n", phrase_test)
print("\n Traduction en français :\n", phrase_traduite)
# 3. Traduire chaque morceau
#traductions = []
#for morceau in morceaux:
    #entrees = tokenizer(morceau, return_tensors="pt", padding=True, truncation=True)
    #sortie = modele.generate(**entrees)
    #texte_fr = tokenizer.batch_decode(sortie, skip_special_tokens=True)[0]
    #traductions.append(texte_fr)
# 4. Recomposer le texte traduit complet
#texte_traduit = "\n\n".join(traductions)
# 5. (Optionnel) Sauvegarder dans un fichier texte
#with open("/content/drive/MyDrive/pinocchio_traduit_fr.txt", "w", encoding="utf-8") as f:
    #f.write(texte_traduit)
#print("Traduction terminée avec succès !")
# Préparation des données pour TF-IDF
documents = [raw]
from sklearn.feature_extraction.text import TfidfVectorizer
import pandas as pd
# Création du modèle TF-IDF (en utilisant les stop words en anglais de sklearn)
vectorizer = TfidfVectorizer(stop_words='english', max_features=20)  # limitation aux 20 mots ayant les scores TF-IDF les plus élevés
matrice_tfidf = vectorizer.fit_transform(documents)
# Extraction du vocabulaire et des scores TF-IDF associés
noms_des_mots = vectorizer.get_feature_names_out()
scores_tfidf = matrice_tfidf.toarray().flatten()
# Création d'un DataFrame avec le vocabulaire et les scores TF-IDF
df_tfidf = pd.DataFrame({
    'Vocabulaire': noms_des_mots,
    'TF-IDF': scores_tfidf
}).sort_values(by='TF-IDF', ascending=False)
# Affichage des résultats
print(df_tfidf)</code></pre>
Il est curieux de faire TF-IDF sur une liste d'un seul document. Quel est la signification des scores obtenus ?
</div>
  </div>
  <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
            delimiters: [
                {left: "$$", right: "$$", display: true},
                {left: "$", right: "$", display: false},
                {left: "\\(", right: "\\)", display: false},
                {left: "\\[", right: "\\]", display: true}
            ]
        });
    });
</script>
</body>
</html>

<!DOCTYPE html>
<html lang="fr">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Navigation Code Cells</title>
  <script src="https://cdn.tailwindcss.com"></script>
  <script src="https://cdn.jsdelivr.net/npm/vue@2"></script>
    <!-- Prism.js CSS + Python support -->
  <link href="https://cdn.jsdelivr.net/npm/prismjs@1/themes/prism-tomorrow.css" rel="stylesheet" />
  <script src="https://cdn.jsdelivr.net/npm/prismjs@1/prism.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/prismjs@1/components/prism-python.min.js"></script>
</head>
<body class="p-4">
  <div id="app">
    <div class="grid grid-cols-3 gap-4">
      <div v-for="(column, ci) in pages[currentPage].cols" :key="ci" class="bg-blue-100 p-2 max-h-full overflow-hidden">
        <div v-for="(cell, k) in column" :key="k" class="mb-4">
          <pre class="rounded-xl overflow-x-auto shadow-md p-2 bg-white"><code class="language-python">{{ cell.join('') }}</code></pre>
        </div>
      </div>
    </div>
    <div class="flex justify-between items-center mt-4">
      <button @click="prevPage"
              :disabled="currentPage===0"
              class="px-4 py-2 bg-gray-500 text-white rounded disabled:opacity-50">
        ← Précédent
      </button>
      <span>Page {{ currentPage+1 }} / {{ pages.length }}</span>
      <button @click="nextPage"
              :disabled="currentPage>=pages.length-1"
              class="px-4 py-2 bg-gray-500 text-white rounded disabled:opacity-50">
        Suivant →
      </button>
    </div>
  </div>

  <script>
  // Injection des données paginées
  const pages = [{"cols": [[["from google.colab import drive\n", "drive.mount('/content/drive')"], ["# Bibliothèque pour lire les fichiers PD\n", "!pip install pymupdf"], ["# Lire le fichier texte brut\n", "with open(\"/content/drive/MyDrive/pinocchio.txt\", \"r\", encoding=\"utf-8\") as f:\n", "    raw = f.read()\n"], ["import re\n", "# Nettoyage de base\n", "raw = re.sub(r'Pinocchio…\\s*\\d+', '', raw)  # Supprime les titres avec numéros de page\n", "raw = re.sub(r'\\n+', '\\n', raw).strip()     # Supprime les lignes vides multiples\n"], ["# Importation de la bibliothèque NLTK (Natural Language Toolkit)\n", "import nltk\n", "# Téléchargement des données nécessaires pour le tokeniseur\n", "nltk.download('punkt')\n", "nltk.download('punkt_tab')\n", "nltk.download('stopwords')\n", "nltk.download('wordnet')\n", "nltk.download('omw-1.4')\n", "nltk.download('averaged_perceptron_tagger')"], ["# Tokenisation du texte brut (variable 'raw') en mots individuels (tokens)\n", "tokens = nltk.word_tokenize(raw)\n", "# Affichage des 20 premiers tokens pour vérifier le résultat\n", "print(\"Les 20 premiers tokens extraits du texte\", tokens[:20])\n", "# Création d'un objet Text de NLTK à partir des tokens pour des analyses textuelles plus avancées\n", "text = nltk.Text(tokens)\n", "# Détection et affichage des collocations (expressions fréquentes de mots associés)\n", "text.collocations()\n", "# Importation de la classe FreqDist pour calculer la fréquence des mots\n", "from nltk.probability import FreqDist\n", "# Création de la distribution de fréquence des tokens\n", "fdist = FreqDist(tokens)\n", "# Affichage d’un graphique cumulatif des 30 mots les plus fréquents\n", "fdist.plot(30, cumulative=True)"]], [["from nltk.tokenize import word_tokenize, sent_tokenize\n", "from itertools import islice\n", "# Tokenisation des phrases\n", "sentences = sent_tokenize(raw, language=\"french\")\n", "# Liste des mots (tokenisation simple)\n", "words = [word for sentence in sentences for word in sentence.split()]\n", "# Création des n-grammes\n", "unigrams = list(set(words))\n", "bigrams = list(zip(words, words[1:]))\n", "trigrams = list(zip(words, words[1:], words[2:]))\n", "# Statistiques de base\n", "print(\"Nombre de phrases :\", len(sentences))\n", "print(\"Nombre de mots :\", len(words))\n", "print(\"Nombre de mots uniques (unigrammes) :\", len(unigrams))\n", "print(\"Nombre de bigrammes :\", len(bigrams))\n", "print(\"Nombre de trigrammes :\", len(trigrams))\n", "# Affichage d'exemples\n", "print(\"Exemples de phrases :\")\n", "for sent in sentences[:3]:\n", "    print(\"-\", sent)\n", "print(\"Exemples de bigrammes :\")\n", "for pair in islice(bigrams, 5):\n", "    print(\"-\", pair)\n"], ["import re\n", "# Créer une liste de mots se terminant par \"ion\"\n", "words_ion = [w for w in tokens if re.search('ion$', w)]\n", "print(words_ion[:30])\n"], ["import matplotlib.pyplot as plt\n", "from collections import Counter\n", "# Longueur de chaque mot dans tout le texte\n", "longueurs_mots = [len(m) for m in tokens]\n", "# Histogramme des longueurs de mots\n", "Counter(longueurs_mots).most_common()\n", "plt.hist(longueurs_mots, bins=range(1, max(longueurs_mots)+1))\n", "plt.title(\"Distribution des longueurs de mots\")\n", "plt.xlabel(\"Longueur du mot\")\n", "plt.ylabel(\"Fréquence\")\n", "plt.show()"]], [["# Calcul de la longueur moyenne des phrases\n", "def simple_tokenize(text):\n", "    return re.findall(r\"\\b\\w+\\b\", text)\n", "# Découpe brute en phrases\n", "phrases_brutes = re.split(r'[.!?]', raw)\n", "longueurs_phrases = [len(simple_tokenize(p)) for p in phrases_brutes if p.strip()]\n", "longueur_moyenne_phrase = sum(longueurs_phrases) / len(longueurs_phrases)\n", "longueur_moyenne_phrase\n"], ["# Filtrage des mots de contenu (en supprimant des mots vides simples)\n", "stopwords = ['the', 'and', 'of', 'to', 'a', 'in', 'is', 'that', ',', '.', '!', '?']\n", "content_words = [w for w in tokens if w.lower() not in stopwords]\n", "print(\"Mots de contenu :\", content_words[:20])\n"], ["from nltk.stem import PorterStemmer, WordNetLemmatizer\n", "stemmer = PorterStemmer()\n", "lemmatizer = WordNetLemmatizer()\n", "# Appliquer stemming et lemmatisation\n", "stems = [stemmer.stem(w) for w in tokens]\n", "lemmes = [lemmatizer.lemmatize(w) for w in tokens]\n", "# Statistiques\n", "print(\"Nombre de stems uniques :\", len(set(stems)))\n", "print(\"Nombre de lemmes uniques :\", len(set(lemmes)))\n", "# Fréquence des lemmes\n", "from nltk.probability import FreqDist\n", "fdist_lem = FreqDist(lemmes)\n", "fdist_lem.plot(30, title=\"Fréquence des lemmes les plus fréquents\")"], ["#pip install transformers"], ["# Découper le texte en petits morceaux pour la traduction\n", "def decouper_texte(text, longueur_max=400):\n", "    phrases = text.split('.')\n", "    morceaux = []\n", "    courant = ''\n", "    for phrase in phrases:\n", "        if len(courant) + len(phrase) < longueur_max:\n", "            courant += phrase + '.'\n", "        else:\n", "            morceaux.append(courant.strip())\n", "            courant = phrase + '.'\n", "    if courant:\n", "        morceaux.append(courant.strip())\n", "    return morceaux\n", "morceaux = decouper_texte(raw)"]]]}, {"cols": [[["# 2. Charger le modèle de traduction anglais → français depuis Hugging Face\n", "from transformers import MarianTokenizer, MarianMTModel\n", "modele_nom = \"Helsinki-NLP/opus-mt-en-fr\"\n", "tokenizer = MarianTokenizer.from_pretrained(modele_nom)\n", "modele = MarianMTModel.from_pretrained(modele_nom)\n"], ["# Tester la traduction sur une seule phrase\n", "phrase_test = raw.split('.')[0].strip() + \".\"\n", "entrees = tokenizer(phrase_test, return_tensors=\"pt\", padding=True, truncation=True)\n", "sortie = modele.generate(**entrees)\n", "phrase_traduite = tokenizer.batch_decode(sortie, skip_special_tokens=True)[0]\n", "print(\" Phrase originale :\\n\", phrase_test)\n", "print(\"\\n Traduction en français :\\n\", phrase_traduite)\n"], ["# 3. Traduire chaque morceau\n", "#traductions = []\n", "#for morceau in morceaux:\n", "    #entrees = tokenizer(morceau, return_tensors=\"pt\", padding=True, truncation=True)\n", "    #sortie = modele.generate(**entrees)\n", "    #texte_fr = tokenizer.batch_decode(sortie, skip_special_tokens=True)[0]\n", "    #traductions.append(texte_fr)"], ["# 4. Recomposer le texte traduit complet\n", "#texte_traduit = \"\\n\\n\".join(traductions)\n", "# 5. (Optionnel) Sauvegarder dans un fichier texte\n", "#with open(\"/content/drive/MyDrive/pinocchio_traduit_fr.txt\", \"w\", encoding=\"utf-8\") as f:\n", "    #f.write(texte_traduit)\n", "#print(\"Traduction terminée avec succès !\")"], ["# Préparation des données pour TF-IDF\n", "documents = [raw]\n", "from sklearn.feature_extraction.text import TfidfVectorizer\n", "import pandas as pd\n", "# Création du modèle TF-IDF (en utilisant les stop words en anglais de sklearn)\n", "vectorizer = TfidfVectorizer(stop_words='english', max_features=20)  # limitation aux 20 mots ayant les scores TF-IDF les plus élevés\n", "matrice_tfidf = vectorizer.fit_transform(documents)\n", "# Extraction du vocabulaire et des scores TF-IDF associés\n", "noms_des_mots = vectorizer.get_feature_names_out()\n", "scores_tfidf = matrice_tfidf.toarray().flatten()\n", "# Création d'un DataFrame avec le vocabulaire et les scores TF-IDF\n", "df_tfidf = pd.DataFrame({\n", "    'Vocabulaire': noms_des_mots,\n", "    'TF-IDF': scores_tfidf\n", "}).sort_values(by='TF-IDF', ascending=False)\n", "# Affichage des résultats\n", "print(df_tfidf)\n"]], [["from sklearn.feature_extraction.text import CountVectorizer\n", "from sklearn.decomposition import LatentDirichletAllocation\n", "# 1. Nettoyage léger du texte brut\n", "def nettoyer_texte(texte):\n", "    texte = texte.lower()\n", "    texte = re.sub(r'\\d+', '', texte)\n", "    texte = re.sub(r'[^\\w\\s]', '', texte)\n", "    texte = re.sub(r'\\s+', ' ', texte)\n", "    return texte\n", "texte_nettoye = nettoyer_texte(raw)\n", "# 2. Découpage du texte en paragraphes ou phrases\n", "documents = texte_nettoye.split('. ')\n", "# Transformer les documents en une matrice document-terme\n", "vectorizer = CountVectorizer(stop_words='english', token_pattern=r'\\b[a-zA-Z]{4,}\\b')\n", "matrice_doc_terme = vectorizer.fit_transform(documents)\n", "# Création du modèle LDA pour identifier 3 sujets latents (n_topics=3)\n", "nombre_de_sujets = 3\n", "modele_lda = LatentDirichletAllocation(n_components=nombre_de_sujets, random_state=42)\n", "modele_lda.fit(matrice_doc_terme)\n", "# Fonction d'affichage des résultats des sujets\n", "def afficher_sujets(modele, noms_caracteristiques, nombre_mots):\n", "    for index, sujet in enumerate(modele.components_):\n", "        print(f\"Sujet #{index + 1}:\")\n", "        indices_mots = sujet.argsort()[:-nombre_mots - 1:-1]\n", "        mots_importants = [noms_caracteristiques[i] for i in indices_mots]\n", "        poids_mots = sujet[indices_mots]\n", "        print(\" | \".join([f\"{mot} ({poids:.2f})\" for mot, poids in zip(mots_importants, poids_mots)]))\n", "        print()\n", "# Afficher les 5 mots les plus représentatifs pour chaque sujet\n", "nombre_mots = 10\n", "noms_caracteristiques = vectorizer.get_feature_names_out()\n", "print(\"Les sujets latents extraits du texte « Pinocchio » :\\n\")\n", "afficher_sujets(modele_lda, noms_caracteristiques, nombre_mots)\n", "# Afficher la distribution des sujets dans chaque paragraphe (les 500 premiers seulement)\n", "distribution_sujets_documents = modele_lda.transform(matrice_doc_terme)\n", "for i, distribution in enumerate(distribution_sujets_documents[:500]):\n", "    info_sujet = \" | \".join([f\"Sujet {j+1}: {prob:.2f}\" for j, prob in enumerate(distribution)])\n", "    print(f\"Paragraphe {i+1}: {info_sujet}\\n\")\n"]], [["from textblob import TextBlob\n", "# Nettoyage du texte brut\n", "def nettoyer_texte(texte):\n", "    texte = texte.replace('\\xa0', ' ')\n", "    texte = re.sub(r'\\s+', ' ', texte)\n", "    return texte.strip()\n", "texte_propre = nettoyer_texte(raw)\n", "# Découpage du texte en phrases\n", "phrases = sent_tokenize(texte_propre)\n", "# Liste des personnages à suivre\n", "personnages = ['pinocchio', 'geppetto', 'fairy', 'cricket', 'fox', 'cat']\n", "# Analyse de chaque phrase\n", "resultats = []\n", "for phrase in phrases:\n", "    phrase_minuscule = phrase.lower()\n", "    personnages_trouves = [p for p in personnages if p in phrase_minuscule]\n", "    if personnages_trouves:  # uniquement si un personnage est mentionné\n", "        polarite = TextBlob(phrase).sentiment.polarity\n", "        sentiment = \"Positif\" if polarite > 0.1 else \"Négatif\" if polarite < -0.1 else \"Neutre\"\n", "        for personnage in personnages_trouves:\n", "            resultats.append({\n", "                \"Phrase\": phrase,\n", "                \"Personnage\": personnage.capitalize(),\n", "                \"Sentiment\": sentiment,\n", "                \"Score\": round(polarite, 3)\n", "            })\n", "# Création du tableau de résultats\n", "df_textblob = pd.DataFrame(resultats)\n", "# Affichage des premières lignes\n", "print(df_textblob.head())\n", "# Exporter les résultats vers un fichier CSV\n", "df_textblob.to_csv(\"sentiments_par_personnage.csv\", index=False)\n"], ["from collections import Counter\n", "# Créer une liste de tuples (Personnage, Sentiment)\n", "combinaisons = list(zip(df_textblob[\"Personnage\"], df_textblob[\"Sentiment\"]))\n", "# Compter les occurrences\n", "compte = Counter(combinaisons)\n", "# Affichage des résultats\n", "print(\"Répartition des sentiments par personnage :\")\n", "for (personnage, sentiment), nb in compte.items():\n", "    print(f\"{personnage} - {sentiment} : {nb} phrases\")\n"]]]}, {"cols": [[["from nltk import word_tokenize\n", "from nltk.classify import NaiveBayesClassifier\n", "# Dữ liệu huấn luyện đơn giản (bạn có thể mở rộng)\n", "train_data = [\n", "    (\"I love this!\", \"Positif\"),\n", "    (\"What a wonderful scene.\", \"Positif\"),\n", "    (\"This is terrible.\", \"Négatif\"),\n", "    (\"I hate that character.\", \"Négatif\"),\n", "    (\"Nothing much happened.\", \"Neutre\"),\n", "    (\"It was okay, not great.\", \"Neutre\")\n", "]\n", "def extract_features(text):\n", "    words = word_tokenize(text)\n", "    return {f\"contains({w.lower()})\": True for w in words}\n", "train_set = [(extract_features(text), label) for (text, label) in train_data]\n", "nb_classifier = NaiveBayesClassifier.train(train_set)"], ["# Analyse avec Naive Bayes\n", "resultats_nb = []\n", "for phrase in phrases:\n", "    phrase_minuscule = phrase.lower()\n", "    personnages_trouves = [p for p in personnages if p in phrase_minuscule]\n", "    if personnages_trouves:\n", "        features = extract_features(phrase)\n", "        sentiment_nb = nb_classifier.classify(features)\n", "        for personnage in personnages_trouves:\n", "            resultats_nb.append({\n", "                \"Phrase\": phrase,\n", "                \"Personnage\": personnage.capitalize(),\n", "                \"Sentiment_NaiveBayes\": sentiment_nb\n", "            })\n", "# Créer DataFrame et sauvegarder\n", "df_nb = pd.DataFrame(resultats_nb)\n", "print(df_nb.head())\n", "df_nb.to_csv(\"sentiments_naivebayes.csv\", index=False)"], ["# Fusionner les deux résultats sur Phrase + Personnage\n", "df_comparaison = pd.merge(df_textblob, df_nb, on=[\"Phrase\", \"Personnage\"])\n", "print(df_comparaison.head())\n", "# Sauvegarder\n", "df_comparaison.to_csv(\"comparaison_sentiments_textblob_vs_nb.csv\", index=False)\n"], ["#Calcul du pourcentage d’accord entre les deux modèles\n", "# Comparer les sentiments prédits\n", "df_comparaison[\"Même sentiment\"] = df_comparaison[\"Sentiment\"] == df_comparaison[\"Sentiment_NaiveBayes\"]\n", "# Calculer le pourcentage d’accord\n", "accord = df_comparaison[\"Même sentiment\"].mean() * 100\n", "print(f\"Pourcentage d’accord entre TextBlob et Naive Bayes : {accord:.2f} %\")\n"]], [["#Matrice de confusion\n", "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n", "# Vrais sentiments (TextBlob) vs prédictions (Naive Bayes)\n", "y_true = df_comparaison[\"Sentiment\"]\n", "y_pred = df_comparaison[\"Sentiment_NaiveBayes\"]\n", "# Définir les étiquettes\n", "labels = [\"Positif\", \"Neutre\", \"Négatif\"]\n", "# Matrice de confusion\n", "cm = confusion_matrix(y_true, y_pred, labels=labels)\n", "# Affichage graphique\n", "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n", "disp.plot(cmap=\"Blues\", xticks_rotation=45)\n", "plt.title(\"Matrice de confusion : TextBlob vs Naive Bayes\")\n", "plt.show()\n"], ["import fitz\n", "from sklearn.linear_model import LogisticRegression\n", "exemples = [\n", "    (\"Pinocchio said he was sorry.\", \"parole\"),\n", "    (\"Geppetto ran after him.\", \"action physique\"),\n", "    (\"He felt very sad and cried.\", \"émotion\"),\n", "    (\"The sun was shining above the hills.\", \"description\"),\n", "    (\"The Fairy asked him why he lied.\", \"parole\"),\n", "    (\"He jumped over the wall.\", \"action physique\"),\n", "    (\"He was afraid of being punished.\", \"émotion\"),\n", "    (\"There was a silence in the room.\", \"description\")\n", "]\n", "X_entrainement_textes = [x[0] for x in exemples]\n", "y_entrainement = [x[1] for x in exemples]\n", "# Vectorisation des textes\n", "vectorizer = TfidfVectorizer()\n", "X_entrainement = vectorizer.fit_transform(X_entrainement_textes)\n", "# Entraînement du modèle\n", "modele = LogisticRegression()\n", "modele.fit(X_entrainement, y_entrainement)\n", "# Prédiction du type d’action sur l’ensemble du texte\n", "X_complet = vectorizer.transform(phrases)\n", "predictions = modele.predict(X_complet)\n", "# Affichage des résultats\n", "df_resultat = pd.DataFrame({\n", "    \"Phrase\": phrases,\n", "    \"Type d'action\": predictions\n", "})\n", "# Afficher les 10 premières lignes\n", "print(df_resultat.head(10))"]], [["compte = Counter(predictions)\n", "print(\"Répartition des types d'action :\")\n", "for action, nb in compte.items():\n", "    print(f\"{action.capitalize()} : {nb} phrases\")"], ["nltk.download('punkt')\n", "nltk.download('averaged_perceptron_tagger')\n", "nltk.download('averaged_perceptron_tagger_eng')\n", "nltk.download('maxent_ne_chunker')\n", "nltk.download('words')\n", "from nltk import word_tokenize, pos_tag, RegexpParser, sent_tokenize\n", "# Définir une grammaire pour repérer les groupes nominaux\n", "grammaire = \"NP: {<DT>?<JJ>*<NN|NNS|NNP|NNPS>+}\"\n", "analyseur = RegexpParser(grammaire)\n", "# Exemple sur une phrase isolée\n", "phrase_exemple = \"The good fairy gave Pinocchio a piece of advice.\"\n", "tokens_exemple = word_tokenize(phrase_exemple)\n", "tags_exemple = pos_tag(tokens_exemple)\n", "arbre_exemple = analyseur.parse(tags_exemple)\n", "print(\"\\nAnalyse syntaxique de la phrase exemple :\")\n", "arbre_exemple.pretty_print()\n", "# Application sur tout le texte ----\n", "phrases = sent_tokenize(raw)\n", "groupes_nominaux = []\n", "for phrase in phrases:\n", "    tokens = word_tokenize(phrase)\n", "    tags = pos_tag(tokens)\n", "    arbre = analyseur.parse(tags)\n", "    for sous_arbre in arbre.subtrees():\n", "        if sous_arbre.label() == 'NP':\n", "            groupe = \" \".join(mot for mot, tag in sous_arbre.leaves())\n", "            groupes_nominaux.append(groupe)\n", "print(\"\\nExemples de groupes nominaux extraits dans le texte :\")\n", "for g in groupes_nominaux[:20]:\n", "    print(\"-\", g)\n", "print(f\"\\nNombre total de groupes nominaux détectés : {len(groupes_nominaux)}\")"]]]}, {"cols": [[["import string\n", "import nltk\n", "from nltk.tokenize import word_tokenize, sent_tokenize\n", "from nltk.corpus import stopwords\n", "from sklearn.feature_extraction.text import TfidfVectorizer\n", "from sklearn.metrics.pairwise import cosine_similarity\n", "from transformers import pipeline\n", "nltk.download('punkt')\n", "nltk.download('stopwords')\n", "# Fonction de prétraitement du texte\n", "stop_words = set(stopwords.words('english'))\n", "def nettoyer_texte(text):\n", "    text = text.lower()\n", "    text = ''.join([c for c in text if c not in string.punctuation])\n", "    tokens = word_tokenize(text)\n", "    tokens = [w for w in tokens if w not in stop_words]\n", "    return ' '.join(tokens)"], ["# Fonction pour trouver la phrase la plus pertinente avec TF-IDF\n", "def repondre_tfidf(question, corpus):\n", "    corpus_nettoye = [nettoyer_texte(sent) for sent in corpus]\n", "    question_nettoyee = nettoyer_texte(question)\n", "    vect = TfidfVectorizer()\n", "    matrice_tfidf = vect.fit_transform(corpus_nettoye + [question_nettoyee])\n", "    vect_question = matrice_tfidf[-1]\n", "    vect_docs = matrice_tfidf[:-1]\n", "    similarites = cosine_similarity(vect_question, vect_docs)[0]\n", "    meilleur_index = similarites.argmax()\n", "    return corpus[meilleur_index], similarites[meilleur_index], meilleur_index\n"], ["# Fonction de réponse avec Hugging Face\n", "modele_qa = pipeline(\"question-answering\")\n", "def repondre_huggingface(question, contexte):\n", "    try:\n", "        resultat = modele_qa(question=question, context=contexte)\n", "        return resultat[\"answer\"], resultat[\"score\"]\n", "    except:\n", "        return \"Je ne sais pas\", 0.0\n"]], [["# Exemple de question\n", "question = \"Qui est Geppetto ?\"\n", "# retrouver la phrase la plus proche\n", "phrase_pertinente, score_tfidf, index_phrase = repondre_tfidf(question, phrases)\n", "# créer un meilleur contexte si la phrase est trop courte\n", "if len(phrase_pertinente.split()) < 8:\n", "    contexte = \" \".join(phrases[index_phrase:index_phrase + 3])\n", "else:\n", "    contexte = phrase_pertinente\n", "# extraction de la réponse\n", "reponse, score_hf = repondre_huggingface(question, contexte)\n", "# Résultats\n", "print(\"Méthode TF-IDF :\")\n", "print(\"→ Phrase la plus pertinente :\", phrase_pertinente)\n", "print(\"→ Similarité :\", round(score_tfidf, 3))\n", "print(\"\\nModèle Hugging Face :\")\n", "print(\"→ Réponse extraite :\", reponse)\n", "print(\"→ Score de confiance :\", round(score_hf, 3))"]], [["# Résumé extractif avec CamemBERT\n", "import numpy as np\n", "import torch\n", "from transformers import AutoTokenizer, AutoModel\n", "from numpy.linalg import norm\n", "# Chargement du tokenizer et du modèle CamemBERT\n", "model_name = \"camembert-base\"\n", "tokenizer = AutoTokenizer.from_pretrained(model_name)\n", "model = AutoModel.from_pretrained(model_name)\n", "# Fonction pour obtenir l'embedding d'une phrase\n", "def get_sentence_embedding(phrase):\n", "    inputs = tokenizer(phrase, return_tensors=\"pt\", truncation=True, max_length=512)\n", "    with torch.no_grad():\n", "        outputs = model(**inputs)\n", "    return outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n", "# Embeddings des phrases\n", "embeddings = [get_sentence_embedding(p) for p in phrases]\n", "doc_embedding = np.mean(embeddings, axis=0)\n", "# Fonction de similarité cosinus\n", "def cosine_similarity(v1, v2):\n", "    return np.dot(v1, v2) / (norm(v1) * norm(v2))\n", "# Score de chaque phrase\n", "scores = [cosine_similarity(e, doc_embedding) for e in embeddings]\n", "# Sélection des meilleures phrases (top 30 %)\n", "nb_phrases = max(1, int(0.3 * len(phrases)))\n", "indices = np.argsort(scores)[-nb_phrases:]\n", "indices = sorted(indices)\n", "# Résumé final\n", "resume = [phrases[i] for i in indices]\n", "# Affichage\n", "print(\"Résumé automatique avec CamemBERT :\\n\")\n", "for ligne in resume:\n", "    print(\"•\", ligne)"], ["# Séparer le texte en chapitres\n", "chapters = re.split(r'CHAPTER \\d+', raw)\n", "chapter_titles = re.findall(r'CHAPTER \\d+', raw)\n", "# Associer chaque titre de chapitre avec son contenu\n", "chapter_dicts = []\n", "for title, content in zip(chapter_titles, chapters[1:]):  # skip the first empty split\n", "    chapter_dicts.append({\"title\": title.strip(), \"content\": content.strip()})"]]]}, {"cols": [[["from transformers import BartTokenizer, BartForConditionalGeneration\n", "# Chargement du modèle de résumé BART\n", "tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n", "model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\")\n", "# Fonction de résumé automatique\n", "def summarize(text):\n", "    inputs = tokenizer(text, return_tensors=\"pt\", max_length=1024, truncation=True)\n", "    summary_ids = model.generate(\n", "        inputs[\"input_ids\"],\n", "        max_length=150,\n", "        min_length=60,\n", "        length_penalty=2.0,\n", "        num_beams=4,\n", "        early_stopping=True\n", "    )\n", "    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)"], ["for i, chapter in enumerate(chapter_dicts):\n", "    print(f\"{chapter['title']}\")\n", "    summary = summarize(chapter['content'])\n", "    print(f\"Summary:\\n{summary}\\n\")"], ["pip install gradio"]], [["import gradio as gr\n", "from transformers import pipeline\n", "# 1. Pipelines\n", "qa_pipeline = pipeline(\"question-answering\")\n", "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n", "translator = pipeline(\"translation_en_to_fr\", model=\"Helsinki-NLP/opus-mt-en-fr\")\n", "sentiment_pipeline = pipeline(\"sentiment-analysis\")\n", "# 2. Fonctions\n", "def poser_question(question):\n", "    contexte = raw[:1500]  # ou résume le texte ou utilise des phrases pertinentes\n", "    reponse = qa_pipeline(question=question, context=contexte)\n", "    return reponse[\"answer\"]\n", "def resumer_texte(input_text):\n", "    resume = summarizer(input_text, max_length=130, min_length=30, do_sample=False)\n", "    return resume[0][\"summary_text\"]\n", "def analyser_sentiment(phrase):\n", "    resultat = sentiment_pipeline(phrase)[0]\n", "    return f\"{resultat['label']} (score: {round(resultat['score'], 2)})\"\n", "def traduire_en_fr(phrase):\n", "    trad = translator(phrase)\n", "    return trad[0]['translation_text']\n", "# 3. Interface Gradio\n", "demo = gr.Interface(\n", "    fn=poser_question,\n", "    inputs=gr.Textbox(label=\"Pose une question sur Pinocchio\"),\n", "    outputs=gr.Textbox(label=\"Réponse\"),\n", "    title=\"Pinocchio QA - Gradio\",\n", "    description=\"Pose une question sur le conte, obtiens une réponse !\"\n", ")\n", "demo2 = gr.Interface(fn=resumer_texte, inputs=\"text\", outputs=\"text\", title=\"Résumé de texte\")\n", "demo3 = gr.Interface(fn=analyser_sentiment, inputs=\"text\", outputs=\"text\", title=\"Analyse de sentiment\")\n", "demo4 = gr.Interface(fn=traduire_en_fr, inputs=\"text\", outputs=\"text\", title=\"Traduction anglais → français\")\n", "# 4. Regrouper tout dans un onglet\n", "gr.TabbedInterface(\n", "    [demo, demo2, demo3, demo4],\n", "    [\"QA\", \"Résumé\", \"Sentiment\", \"Traduction\"]\n", ").launch()\n"]], []]}];

  new Vue({
    el: '#app',
    data: {
      pages: pages,
      currentPage: 0
    },
    methods: {
      nextPage() {
        if (this.currentPage < this.pages.length - 1) {
          this.currentPage++;
        }
      },
      prevPage() {
        if (this.currentPage > 0) {
          this.currentPage--;
        }
      }
    }
  });
  </script>
</body>
</html>
